{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48703e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The code imports three libraries: cv2 (OpenCV), numpy, and matplotlib.pyplot.\n",
    "\n",
    "cv2 is a library for computer vision tasks, including image and video processing.\n",
    "numpy is a library for numerical computations in Python, providing support for arrays and mathematical operations.\n",
    "matplotlib.pyplot is a plotting library used for creating visualizations and graphs.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d65c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847aa3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The code net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\") reads a pre-trained YOLOv3 (You Only Look Once) model from the-\n",
    "specified weights and configuration files.\n",
    "\n",
    "Here's a breakdown of the function call:\n",
    "\n",
    "cv2.dnn refers to the deep neural network module in OpenCV.\n",
    "readNet() is a function within the cv2.dnn module used to load a neural network model.\n",
    "\"yolov3.weights\" is the filename of the pre-trained weights file for the YOLOv3 model.\n",
    "\"yolov3.cfg\" is the filename of the configuration file that defines the architecture and settings of the YOLOv3 model.\n",
    "By calling cv2.dnn.readNet(), the function loads the YOLOv3 model from the weights and configuration files and assigns it to- \n",
    " the variable net. This allows you to use the loaded model for object detection or other computer vision tasks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv3 weights and configuration files\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ac92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The code snippet reads the contents of a file named \"coco.names\" and stores each line of the file as a string element in the-\n",
    " classes list. It then prints the contents of the classes list.\n",
    "\n",
    "Here's a breakdown of the code:\n",
    "\n",
    "classes = []: Initializes an empty list called classes to store the names of the classes.\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:: Opens the file named \"coco.names\" in read mode. The file is opened using a context manager,\n",
    " which ensures that the file is properly closed after it has been read.\n",
    "\n",
    "classes = [line.strip() for line in f.readlines()]: Reads all the lines from the file and creates a list comprehension. Each-\n",
    " line is stripped of leading and trailing whitespaces using the strip() method, and the resulting string is added to the-\n",
    " classes list.\n",
    "\n",
    "print(classes): Prints the contents of the classes list, which now contains the names of classes read from the \"coco.names\"-\n",
    " file.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a11c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class names\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660cd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The code snippet retrieves the names of the output layers of a neural network model loaded in the variable `net` using YOLOv3-\n",
    " architecture.\n",
    "\n",
    "Here's a breakdown of the code:\n",
    "\n",
    "1. `layer_names = net.getLayerNames()`: `net.getLayerNames()` returns a list of all the names of the layers in the neural-\n",
    "    network model. The code assigns this list to the variable `layer_names`.\n",
    "\n",
    "2. `output_layer_names = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]`: `net.getUnconnectedOutLayers()` returns-\n",
    "    the indices of the output layers of the model. The code then creates a new list called `output_layer_names`, where each-\n",
    "    element is the name of the corresponding output layer obtained from `layer_names`. The `-1` adjustment in indexing is-\n",
    "    necessary because layer indices start from 1 in OpenCV, while list indices start from 0.\n",
    "\n",
    "3. `print(output_layer_names)`: Prints the names of the output layers. These output layers are typically the final layers of-\n",
    "    the neural network that produce the predictions or feature maps used for further processing, such as object detection or-\n",
    "    classification.\n",
    "\n",
    "Overall, this code is used to identify and retrieve the names of the output layers in the YOLOv3 model loaded in `net`. The-\n",
    " output layer names are often important for extracting the relevant information from the model's output during inference.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input and output layers\n",
    "#layer_names = net.getLayerNames()\n",
    "#try:\n",
    "#    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "#except:\n",
    "#    print(\"Error: Failed to get unconnected output layers\")\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layer_names = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "print(output_layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acde07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The code performs preprocessing steps on an image before feeding it into a neural network for object detection or\n",
    " other computer vision tasks using the YOLOv3 model.\n",
    "\n",
    "Here's a breakdown of the code:\n",
    "\n",
    "img = cv2.imread(\"img1.jpg\"): Reads and loads an image file named \"img1.jpg\" using the cv2.imread() function. The loaded image\n",
    " is stored in the img variable.\n",
    "\n",
    "height, width, channels = img.shape: Obtains the height, width, and number of channels (color channels) of the image using the\n",
    " shape attribute of the img variable. This information is stored in separate variables for later use.\n",
    "\n",
    " blob = cv2.dnn.blobFromImage(img, 1/255.0, (416, 416), swapRB=True, crop=False): Preprocesses the image into a format suitable\n",
    " for inputting into the YOLOv3 model. The cv2.dnn.blobFromImage() function takes the following arguments:\n",
    "\n",
    "img: The input image to be preprocessed.\n",
    "1/255.0: The scaling factor to normalize pixel values in the range of 0 to 1.\n",
    "(416, 416): The size to which the input image is resized.\n",
    "swapRB=True: Specifies whether to swap the color channels from BGR (Blue-Green-Red) to RGB.\n",
    "crop=False: Determines whether to crop the image or pad it with zeros to match the desired input size.\n",
    "The function applies the necessary preprocessing operations, such as resizing, normalization, and channel swapping, to convert\n",
    " the image into a blob. A blob is a multi-dimensional array that can be efficiently processed by the neural network.\n",
    "\n",
    "The resulting blob is assigned to the variable blob and can be passed as input to the YOLOv3 model for object detection or\n",
    " other tasks.\n",
    "\n",
    "Overall, this code reads an image file, obtains its dimensions, and preprocesses it into a blob format suitable for feeding\n",
    " into the YOLOv3 model.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and prepare for inference\n",
    "img = cv2.imread(\"D:\\\\College\\\\6th\\\\Project\\\\cats and dogs\\\\test\\\\dogs\\\\dog_123.jpg\")\n",
    "height, width, channels = img.shape\n",
    "blob = cv2.dnn.blobFromImage(img, 1/255.0, (416, 416), swapRB=True, crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924979e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The code sets the input of a neural network model loaded in the variable `net` with a preprocessed image blob \n",
    " and performs forward propagation to obtain the output predictions or feature maps from the specified output layers.\n",
    "\n",
    " Here's a breakdown of the code:\n",
    "\n",
    " 1. `net.setInput(blob)`: Sets the input of the neural network model `net` with the preprocessed image blob (`blob`) obtained \n",
    "     from the previous step. This step prepares the model to process the input image.\n",
    "\n",
    " 2. `outs = net.forward(output_layer_names)`: Performs forward propagation on the neural network model `net` using the \n",
    "     specified output layer names (`output_layer_names`). This step propagates the input image forward through the model and \n",
    "     computes the output predictions or feature maps from the specified output layers.\n",
    "\n",
    "    The `net.forward()` method takes the output layer names as an argument and returns the output values. These output values \n",
    " contain the predictions or feature maps generated by the model for the given input image.\n",
    "\n",
    "    The resulting output is assigned to the variable `outs`, which can be further processed and analyzed to obtain the desired \n",
    "     results, such as object detection bounding boxes, class probabilities, or other relevant information.\n",
    "\n",
    " Overall, this code sets the input of the neural network model with a preprocessed image blob and performs forward propagation \n",
    " to obtain the output predictions or feature maps from the specified output layers. This is a crucial step in the inference \n",
    " process of object detection or other computer vision tasks using the YOLOv3 model.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b137fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass image through the network\n",
    "net.setInput(blob)\n",
    "outs = net.forward(output_layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca29a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The code processes the output of the neural network model to extract the relevant information about the detected \n",
    " objects, including their bounding boxes, class IDs, and confidences.\n",
    "\n",
    " Here's a breakdown of the code:\n",
    "\n",
    " 1. `class_ids = []`, `confidences = []`, `boxes = []`: Initializes three empty lists to store the class IDs, confidences, \n",
    "     and bounding boxes of the detected objects.\n",
    "\n",
    " 2. `for out in outs:`: Iterates over the outputs (`outs`) obtained from the neural network model for each specified output \n",
    "         layer.\n",
    "\n",
    " 3. `for detection in out:`: Iterates over each detection in the current output.\n",
    "\n",
    " 4. `scores = detection[5:]`: Extracts the confidence scores for each class from the detection output. The first five elements \n",
    "     of the `detection` array typically represent the bounding box coordinates and other information.\n",
    "\n",
    " 5. `class_id = np.argmax(scores)`: Determines the class ID with the highest confidence by finding the index of the maximum \n",
    "     score in the `scores` array using `np.argmax()`.\n",
    "\n",
    " 6. `confidence = scores[class_id]`: Retrieves the confidence score corresponding to the selected class ID.\n",
    "\n",
    " 7. `if confidence > 0.5:`: Checks if the confidence score is above a certain threshold (0.5 in this case). If the condition \n",
    "         is met, it means an object of interest has been detected.\n",
    "\n",
    " 8. The following lines calculate and store the information related to the detected object:\n",
    "    - The coordinates of the center (`center_x` and `center_y`) of the bounding box relative to the width and height of the \n",
    " input image.\n",
    "    - The width (`w`) and height (`h`) of the bounding box relative to the width and height of the input image.\n",
    "    - The top-left corner (`x` and `y`) of the bounding box by subtracting half of the width and height from the center \n",
    " coordinates.\n",
    "    - The bounding box coordinates (`x`, `y`, `w`, `h`) are appended to the `boxes` list.\n",
    "    - The confidence score is converted to a float and appended to the `confidences` list.\n",
    "    - The class ID is appended to the `class_ids` list.\n",
    "\n",
    " Overall, this code processes the output of the neural network model, filters out detections below a certain confidence \n",
    " threshold, and extracts the class IDs, confidences, and bounding box coordinates of the detected objects. This information \n",
    " can be further utilized for tasks such as drawing bounding boxes, labeling objects, or performing additional analysis.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process detections\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            # Object detected\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c6029",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The code applies Non-Maximum Suppression (NMS) to filter out overlapping bounding boxes and keep only the most \n",
    " confident and non-overlapping detections.\n",
    "\n",
    " Here's a breakdown of the code:\n",
    "\n",
    " 1. `indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)`: The `cv2.dnn.NMSBoxes()` function takes the following \n",
    "     arguments:\n",
    "\n",
    "    - `boxes`: A list of bounding box coordinates (`[x, y, w, h]`) for the detected objects.\n",
    "    - `confidences`: A list of confidence scores for the detected objects.\n",
    "    - `0.5`: The threshold value used for deciding whether two bounding boxes overlap significantly. If the Intersection over \n",
    "     Union (IoU) between two boxes is above this threshold, the box with the lower confidence score is suppressed.\n",
    "    - `0.4`: The threshold value used to determine the minimum confidence score required for a detection to be considered \n",
    "     during NMS. Detections with confidence scores below this threshold are discarded.\n",
    "\n",
    "    The `cv2.dnn.NMSBoxes()` function performs NMS and returns the indexes of the bounding boxes that survived the suppression \n",
    "     process, indicating the indexes of the selected detections that are non-overlapping and above the confidence threshold.\n",
    "\n",
    "    The resulting `indexes` variable contains the filtered indexes of the bounding boxes that passed the NMS step and can be \n",
    "    used to retrieve the corresponding bounding boxes, confidences, and class IDs for further processing or visualization.\n",
    "\n",
    " Overall, this code applies NMS to the detected bounding boxes and their associated confidences, discarding overlapping boxes \n",
    " and keeping the most confident and non-overlapping detections based on the specified thresholds.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26356e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply non-maximum suppression to remove overlapping boxes\n",
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The code performs visual annotations on the original image by drawing bounding boxes around the detected objects \n",
    " and overlaying text labels with class names and confidence scores.\n",
    "\n",
    " Here's a breakdown of the code:\n",
    "\n",
    " 1. `font = cv2.FONT_HERSHEY_PLAIN`: Specifies the font type for the text annotations.\n",
    "\n",
    " 2. `colors = np.random.uniform(0, 255, size=(len(classes), 3))`: Generates an array of random colors, where each color is \n",
    "     represented by an RGB value. The array has a size of `(number of classes, 3)`, and each class will be assigned a random \n",
    "     color.\n",
    "\n",
    " 3. `if len(indexes) > 0:`: Checks if there are any indexes (detections) after NMS has been applied.\n",
    "\n",
    " 4. `for i in indexes.flatten():`: Iterates over each index (detection) obtained from the filtered indexes.\n",
    "\n",
    " 5. `x, y, w, h = boxes[i]`: Retrieves the bounding box coordinates (`x`, `y`, `w`, `h`) of the current detection using the \n",
    "     index `i`.\n",
    "\n",
    " 6. `label = classes[class_ids[i]]`: Retrieves the class label name corresponding to the class ID of the current detection \n",
    "     using the index `i`.\n",
    "\n",
    " 7. `confidence = confidences[i]`: Retrieves the confidence score of the current detection using the index `i`.\n",
    "\n",
    " 8. `color = colors[class_ids[i]]`: Retrieves the color assigned to the current class ID.\n",
    "\n",
    " 9. `cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)`: Draws a rectangle around the object using the calculated bounding box \n",
    "     coordinates. The rectangle is drawn on the `img` image, with the specified color and line thickness of 2.\n",
    "\n",
    " 10. `text = f\"{label} {confidence:.2f}\"`: Creates a text string that combines the class label and confidence score, formatted \n",
    "     to two decimal places.\n",
    "\n",
    " 11. `cv2.putText(img, text, (x, y-5), font, 2, color, 2)`: Overlays the text string on the `img` image, positioned slightly \n",
    "     above the top-left corner of the bounding box. The text is displayed using the specified font, font size of 2, color, and \n",
    "     line thickness of 2.\n",
    "\n",
    " 12. `print(text)`: Prints the text string, which includes the class label and confidence score, for debugging or information \n",
    "     purposes.\n",
    "\n",
    " Overall, this code iterates over the filtered indexes of the bounding boxes, retrieves the necessary information for each \n",
    " detection, and adds visual annotations to the original image by drawing bounding boxes and overlaying text labels. The result \n",
    " is an image with annotated detections displayed on it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw boxes and labels on image\n",
    "#font = cv2.FONT_HERSHEY_PLAIN\n",
    "#colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "#if len(indexes) > 0:\n",
    "#    for i in indexes.flatten():\n",
    "#        x, y, w, h = boxes[i]\n",
    "#        label = classes[class_ids[i]]\n",
    "#        confidence = confidences[i]\n",
    "#        color = colors[class_ids[i]]\n",
    "#        cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)\n",
    "#        cv2.putText(img, f\"{label} {confidence:.2f}\", (x, y-5), font, 1, color, 1)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "if len(indexes) > 0:\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = classes[class_ids[i]]\n",
    "        confidence = confidences[i]\n",
    "        color = colors[class_ids[i]]\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)\n",
    "        text = f\"{label} {confidence:.2f}\"\n",
    "        cv2.putText(img, text, (x, y-5), font, 2, color, 2)\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The code displays the annotated image using two different methods: using OpenCV's `imshow()` function and using \n",
    "     matplotlib's `imshow()` function from the pyplot module.\n",
    "\n",
    " Here's a breakdown of the code:\n",
    "\n",
    " 1. `cv2.imshow(\"Image\", img)`: Displays the annotated image using OpenCV's `imshow()` function. The first argument `\"Image\"` \n",
    "     specifies the window name, and the second argument `img` is the image to be displayed. This function opens a new window \n",
    "     and shows the image with the applied annotations.\n",
    "\n",
    " 2. `cv2.waitKey(0)`: Pauses the program and waits for a key press. This line ensures that the image window remains open until \n",
    "     a key is pressed.\n",
    "\n",
    " 3. `cv2.destroyAllWindows()`: Closes all open windows created by OpenCV. This line ensures that all image windows are closed \n",
    "     after the `waitKey()` function returns.\n",
    "\n",
    " 4. `img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)`: Converts the image from the BGR color space (used by OpenCV) to the RGB \n",
    "     color \n",
    "     space (used by matplotlib). This conversion is necessary because OpenCV and matplotlib use different color channel orders.\n",
    "\n",
    " 5. `plt.imshow(img)`: Displays the annotated image using matplotlib's `imshow()` function from the pyplot module. This \n",
    "     function displays the image in a new window or notebook cell, using the RGB color space.\n",
    "\n",
    " 6. `plt.show()`: Shows the image in the matplotlib window or notebook cell. This line ensures that the image is displayed.\n",
    "\n",
    " Overall, these code snippets allow you to visualize the annotated image using both OpenCV and matplotlib. The `imshow()` \n",
    " functions are used to display the image, while `waitKey()` and `destroyAllWindows()` ensure the windows are handled correctly \n",
    " when using OpenCV. The conversion from BGR to RGB is necessary to display the image correctly using matplotlib.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859cb2f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display image\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc81d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
